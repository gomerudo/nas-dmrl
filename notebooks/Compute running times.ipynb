{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the running time per experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T01:58:02.988162Z",
     "start_time": "2019-09-17T01:58:02.136927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import for interactive notebook (see:\n",
    "# https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html)\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "\n",
    "\n",
    "# Import to list files in directories\n",
    "import glob\n",
    "\n",
    "# Import for regular expressions\n",
    "import re\n",
    "\n",
    "# Imports for path operations\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# For date operations\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', -1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import configparser\n",
    "\n",
    "# import jtplot module in notebook\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "jtplot.style(theme='onedork')\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T01:58:03.043224Z",
     "start_time": "2019-09-17T01:58:02.989846Z"
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"/Users/gomerudo/workspace/thesis_results\"\n",
    "\n",
    "def rettext(text):\n",
    "    return text\n",
    "\n",
    "def search_in_file(file, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    results = []\n",
    "    for i, line in enumerate(open(file)):\n",
    "        for match in re.finditer(pattern, line):\n",
    "            results.append(match.groups())\n",
    "    return results\n",
    "\n",
    "form_item_layout = Layout(\n",
    "    width=\"50%\"\n",
    ")\n",
    "\n",
    "w_resdirs = interactive(\n",
    "    rettext,\n",
    "#     text=sorted(glob.glob(\"{dir}/[mix-]?[0-9]*\".format(dir=RESULTS_DIR))),\n",
    "    text=sorted(glob.glob(\"{dir}/*\".format(dir=RESULTS_DIR))),\n",
    "    layout=form_item_layout\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T01:58:06.023639Z",
     "start_time": "2019-09-17T01:58:05.986424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a0098be6cc4aeda8eecc7b61e13470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text', options=('/Users/gomerudo/workspace/thesis_results/27969', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w_resdirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:04:38.356453Z",
     "start_time": "2019-09-17T02:04:38.099305Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############ OBTAIN THE FILES AND DIRECTORIES TO QUERY FOR ANALYSIS ############\n",
    "################################################################################\n",
    "\n",
    "# Obtain the chosen directory\n",
    "chosen_dir = w_resdirs.result\n",
    "\n",
    "# experiments dir\n",
    "exp_dir = glob.glob(\"{dir}/experiment*[!.zip]\".format(dir=chosen_dir))[0]\n",
    "\n",
    "# This is a list of all openai dirs, sorted by name (hence, by timestamp)\n",
    "openai_dirs = sorted(glob.glob(\"{dir}/openai*[!.zip]\".format(dir=exp_dir)))\n",
    "\n",
    "# A simple DB of experiments and actions_info.csv should be there\n",
    "dbexp_file = glob.glob(\"{dir}/db_experiments.csv\".format(dir=exp_dir))[0]\n",
    "ainfo_file = glob.glob(\"{dir}/actions_info.csv\".format(dir=exp_dir))[0]\n",
    "config_file = glob.glob(\"{dir}/config*.ini\".format(dir=exp_dir))[0]\n",
    "flog_file = glob.glob(\"{dir}/sl*\".format(dir=chosen_dir))[0]\n",
    "\n",
    "# Make dataframes for the db of experiments and the actions summary\n",
    "dbexp_df = pd.read_csv(dbexp_file)\n",
    "ainfo_df = pd.read_csv(ainfo_file)\n",
    "\n",
    "# Make de target directory\n",
    "import os\n",
    "summaries_dir = \"{exp}/summary\".format(exp=chosen_dir)\n",
    "if not os.path.isdir(summaries_dir):\n",
    "    os.mkdir(summaries_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T01:59:52.935901Z",
     "start_time": "2019-09-17T01:59:52.925674Z"
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# ########### BUILD THE RELEVANT DATA FRAMES TO PRINT FOR MAIN SUMMARY ###########\n",
    "# ################################################################################\n",
    "    \n",
    "# # Try to obtain the current times\n",
    "# # running_times = search_in_file(flog_file, \".*\\s+(.*)elapsed\")\n",
    "# # if len(running_times) == len(openai_dirs):\n",
    "# #     f_running_times = []\n",
    "# #     for time in running_times:\n",
    "# #         time_cleansed = time[0].split(\".\")[0]\n",
    "# #         f_running_times.append(time_cleansed)\n",
    "# # else:\n",
    "# # prev_timestamp = 0\n",
    "# f_running_times = []\n",
    "# for directory in openai_dirs:\n",
    "#     exp_dirname_only = os.path.basename(directory)\n",
    "#     timestamp = os.path.basename(exp_dirname_only.split(\"-\")[1])\n",
    "#     d2 = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "#     if prev_timestamp:  # 2019 05 29 211533\n",
    "#         d1 = datetime.strptime(prev_timestamp, \"%Y%m%d%H%M%S\")\n",
    "#         f_running_times.append(str(d2 - d1))\n",
    "#     prev_timestamp = timestamp\n",
    "# f_running_times.append(\"NA\")\n",
    "\n",
    "# openai_dirs_df = pd.DataFrame(zip(openai_dirs, f_running_times), columns=[\"Log directory\", \"Runtime\"])\n",
    "\n",
    "# # 4. Search all exceptions\n",
    "# exceptions_all = search_in_file(flog_file, \"failed with exception of type.*<(.*)>.*Message.*:\\s*(.*)\")\n",
    "# n_exceptions = len(exceptions_all)\n",
    "\n",
    "# exceptions_set = set()\n",
    "# for error, message in exceptions_all:\n",
    "#     exceptions_set.add(error)\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "\n",
    "# _ = config.read(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "ainfo_df.shape[0]": "<p><strong>NameError</strong>: name &#39;ainfo_df&#39; is not defined</p>\n",
     "chosen_dir": "<p><strong>NameError</strong>: name &#39;chosen_dir&#39; is not defined</p>\n",
     "config['DEFAULT']['LogPath']": "UsageError: Invalid config statement: &quot;[&#39;DEFAULT&#39;][&#39;LogPath&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['bash']['Algorithm']": "UsageError: Invalid config statement: &quot;[&#39;bash&#39;][&#39;Algorithm&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['bash']['Environment']": "UsageError: Invalid config statement: &quot;[&#39;bash&#39;][&#39;Environment&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['bash']['NSteps']": "UsageError: Invalid config statement: &quot;[&#39;bash&#39;][&#39;NSteps&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['bash']['Network']": "UsageError: Invalid config statement: &quot;[&#39;bash&#39;][&#39;Network&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['bash']['NumTimesteps']": "UsageError: Invalid config statement: &quot;[&#39;bash&#39;][&#39;NumTimesteps&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['metadataset']['DatasetID']": "UsageError: Invalid config statement: &quot;[&#39;metadataset&#39;][&#39;DatasetID&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['metadataset']['TFRecordsRootDir']": "UsageError: Invalid config statement: &quot;[&#39;metadataset&#39;][&#39;TFRecordsRootDir&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['nasenv.default']['ActionSpaceType']": "UsageError: Invalid config statement: &quot;[&#39;nasenv.default&#39;][&#39;ActionSpaceType&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['nasenv.default']['ConfigFile']": "UsageError: Invalid config statement: &quot;[&#39;nasenv.default&#39;][&#39;ConfigFile&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['nasenv.default']['DatasetHandler']": "UsageError: Invalid config statement: &quot;[&#39;nasenv.default&#39;][&#39;DatasetHandler&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['nasenv.default']['DbFile']": "UsageError: Invalid config statement: &quot;[&#39;nasenv.default&#39;][&#39;DbFile&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['nasenv.default']['MaxSteps']": "UsageError: Invalid config statement: &quot;[&#39;nasenv.default&#39;][&#39;MaxSteps&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['nasenv.default']['TrainerType']": "UsageError: Invalid config statement: &quot;[&#39;nasenv.default&#39;][&#39;TrainerType&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['trainer.default']['BatchSize']": "UsageError: Invalid config statement: &quot;[&#39;trainer.default&#39;][&#39;BatchSize&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "config['trainer.default']['NEpochs']": "",
     "config['trainer.tensorflow']['EnableDistributed']": "UsageError: Invalid config statement: &quot;[&#39;trainer.tensorflow&#39;][&#39;EnableDistributed&#39;]&quot;, should be <code>Class.trait = value</code>.",
     "flog_file": "<p><strong>NameError</strong>: name &#39;flog_file&#39; is not defined</p>\n",
     "n_exceptions": "<p><strong>NameError</strong>: name &#39;n_exceptions&#39; is not defined</p>\n",
     "openai_dirs_df": "<p><strong>NameError</strong>: name &#39;openai_dirs_df&#39; is not defined</p>\n",
     "pd.DataFrame(exceptions_set, columns = [\"Error type\"])": "<p><strong>NameError</strong>: name &#39;exceptions_set&#39; is not defined</p>\n"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- **Chosen results directory is:** {{chosen_dir}}\n",
    "- **Full log is available at:** {{flog_file}}\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "- **Log Path:** {{config['DEFAULT']['LogPath']}}\n",
    "- **Environment:** {{config['bash']['Environment']}}\n",
    "\n",
    "##### Reinforcement Learning\n",
    "\n",
    "- **Algorithm:** {{config['bash']['Algorithm']}}\n",
    "- **Policy representation:** {{config['bash']['Network']}}\n",
    "- **Number of steps:** {{config['bash']['NSteps']}}\n",
    "- **Total number of timestamps:** {{config['bash']['NumTimesteps']}}\n",
    "- **Number of actions:** {{ainfo_df.shape[0]}}\n",
    "\n",
    "##### NAS details\n",
    "\n",
    "- **Config file:** {{config['nasenv.default']['ConfigFile']}}\n",
    "- **Max Steps:** {{config['nasenv.default']['MaxSteps']}}\n",
    "- **DB of experiments:** {{config['nasenv.default']['DbFile']}}\n",
    "- **Dataset Handler:** {{config['nasenv.default']['DatasetHandler']}}\n",
    "- **Action Space Type:** {{config['nasenv.default']['ActionSpaceType']}}\n",
    "- **Trainer:** {{config['nasenv.default']['TrainerType']}}\n",
    "\n",
    "##### Training details\n",
    "\n",
    "- **Batch size:** {{config['trainer.default']['BatchSize']}}\n",
    "- **Epochs:** {{config['trainer.default']['NEpochs']}}\n",
    "- **Distributed:** {{config['trainer.tensorflow']['EnableDistributed']}}\n",
    "\n",
    "##### Meta-dataset details\n",
    "\n",
    "- **TFRecordsRootDir:** {{config['metadataset']['TFRecordsRootDir']}}\n",
    "- **DatasetID:** {{config['metadataset']['DatasetID']}}\n",
    "\n",
    "#### Individual run directories/time\n",
    "\n",
    "{{openai_dirs_df}}\n",
    "\n",
    "#### Errors found in log while building networks\n",
    "\n",
    "- **Total number of exceptions:** {{n_exceptions}}\n",
    "\n",
    "{{pd.DataFrame(exceptions_set, columns = [\"Error type\"])}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:02:23.083131Z",
     "start_time": "2019-09-17T02:02:23.065216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read the episode_logs in /Users/gomerudo/workspace/thesis_results/32261/experiment-20190910012715/openai-20190910012715\n",
      "0:00:00\n",
      "Could not read the episode_logs in /Users/gomerudo/workspace/thesis_results/32261/experiment-20190910012715/openai-20190910012715\n",
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "def trial_summary(trial_log, include_repeated=True):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    total_runtime = 0 \n",
    "    trial_df = pd.read_csv(trial_log)\n",
    "#     trial_df = trial_df[] # Random search only\n",
    "    all_archs = set()\n",
    "    # Iterate the log\n",
    "    for idx, row in trial_df.iterrows():\n",
    "        # Obtain the information information\n",
    "        arch_id = row['composed_id']\n",
    "        running_time = int(row['running_time'])\n",
    "        running_time = 0 if not include_repeated and arch_id in all_archs else running_time\n",
    "        total_runtime += running_time\n",
    "        # add to list at the end\n",
    "        all_archs.add(arch_id)\n",
    "    return total_runtime\n",
    "\n",
    "# Obtain statistics for each trial \n",
    "times = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "        r_time = trial_summary(trial_log, True)\n",
    "        times.append(r_time)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "\n",
    "total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=total_time)))\n",
    "\n",
    "times = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "        r_time = trial_summary(trial_log, False)\n",
    "        times.append(r_time)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "\n",
    "reduced_total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=reduced_total_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:00:15.046241Z",
     "start_time": "2019-09-17T02:00:15.027126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read the episode_logs in /Users/gomerudo/workspace/thesis_results/32261/experiment-20190910012715/openai-20190910012715\n",
      "0:00:00\n",
      "Could not read the episode_logs in /Users/gomerudo/workspace/thesis_results/32261/experiment-20190910012715/openai-20190910012715\n",
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "def trial_summary(trial_log, include_repeated=True):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    total_runtime = 0 \n",
    "    trial_df = pd.read_csv(trial_log)\n",
    "    trial_df = trial_df[:6000] # Random search only\n",
    "    all_archs = set()\n",
    "    # Iterate the log\n",
    "    for idx, row in trial_df.iterrows():\n",
    "        # Obtain the information information\n",
    "        arch_id = row['composed_id']\n",
    "        running_time = int(row['running_time'])\n",
    "        running_time = 0 if not include_repeated and arch_id in all_archs else running_time\n",
    "        total_runtime += running_time\n",
    "        # add to list at the end\n",
    "        all_archs.add(arch_id)\n",
    "    return total_runtime\n",
    "\n",
    "# Obtain statistics for each trial \n",
    "times = []\n",
    "# for i, openai_dir in enumerate(openai_dirs):\n",
    "try:\n",
    "    trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=exp_dir)))[0]\n",
    "    r_time = trial_summary(trial_log, True)\n",
    "    times.append(r_time)\n",
    "except IndexError:\n",
    "    print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "#         pass\n",
    "\n",
    "total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=total_time)))\n",
    "\n",
    "times = []\n",
    "# for i, openai_dir in enumerate(openai_dirs):\n",
    "try:\n",
    "#         trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "    trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=exp_dir)))[0]\n",
    "    r_time = trial_summary(trial_log, False)\n",
    "    times.append(r_time)\n",
    "except IndexError:\n",
    "    print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "    pass\n",
    "\n",
    "reduced_total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=reduced_total_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:04:41.986896Z",
     "start_time": "2019-09-17T02:04:41.453617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 days, 17:07:47\n",
      "2 days, 22:04:14\n"
     ]
    }
   ],
   "source": [
    "def trial_summary(trial_log, include_repeated=True):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    total_runtime = 0 \n",
    "    trial_df = pd.read_csv(trial_log)\n",
    "#     trial_df = trial_df[] # Random search only\n",
    "    all_archs = set()\n",
    "    # Iterate the log\n",
    "    for idx, row in trial_df.iterrows():\n",
    "        # Obtain the information information\n",
    "        arch_id = row['composed_id']\n",
    "        running_time = int(row['running_time'])\n",
    "        running_time = 0 if not include_repeated and arch_id in all_archs else running_time\n",
    "        total_runtime += running_time\n",
    "        # add to list at the end\n",
    "        all_archs.add(arch_id)\n",
    "    return total_runtime\n",
    "\n",
    "# Obtain statistics for each trial \n",
    "times = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/play_logs/*\".format(dir=openai_dir)))[0]\n",
    "        r_time = trial_summary(trial_log, True)\n",
    "        times.append(r_time)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "\n",
    "total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=total_time)))\n",
    "\n",
    "times = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/play_logs/*\".format(dir=openai_dir)))[0]\n",
    "        r_time = trial_summary(trial_log, False)\n",
    "        times.append(r_time)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "\n",
    "reduced_total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=reduced_total_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
