{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the running time per experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:39:21.580451Z",
     "start_time": "2019-09-09T11:39:21.568907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import for interactive notebook (see:\n",
    "# https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html)\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "\n",
    "\n",
    "# Import to list files in directories\n",
    "import glob\n",
    "\n",
    "# Import for regular expressions\n",
    "import re\n",
    "\n",
    "# Imports for path operations\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# For date operations\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', -1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import configparser\n",
    "\n",
    "# import jtplot module in notebook\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "jtplot.style(theme='onedork')\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:39:22.172604Z",
     "start_time": "2019-09-09T11:39:22.123885Z"
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"/Users/gomerudo/workspace/thesis_results\"\n",
    "\n",
    "def rettext(text):\n",
    "    return text\n",
    "\n",
    "def search_in_file(file, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    results = []\n",
    "    for i, line in enumerate(open(file)):\n",
    "        for match in re.finditer(pattern, line):\n",
    "            results.append(match.groups())\n",
    "    return results\n",
    "\n",
    "form_item_layout = Layout(\n",
    "    width=\"50%\"\n",
    ")\n",
    "\n",
    "w_resdirs = interactive(\n",
    "    rettext,\n",
    "#     text=sorted(glob.glob(\"{dir}/[mix-]?[0-9]*\".format(dir=RESULTS_DIR))),\n",
    "    text=sorted(glob.glob(\"{dir}/*\".format(dir=RESULTS_DIR))),\n",
    "    layout=form_item_layout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:39:22.931762Z",
     "start_time": "2019-09-09T11:39:22.896291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6486f2830fa34f30983515a1f3912892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text', options=('/Users/gomerudo/workspace/thesis_results/27969', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w_resdirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:50:49.128559Z",
     "start_time": "2019-09-09T11:50:49.079819Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############ OBTAIN THE FILES AND DIRECTORIES TO QUERY FOR ANALYSIS ############\n",
    "################################################################################\n",
    "\n",
    "# Obtain the chosen directory\n",
    "chosen_dir = w_resdirs.result\n",
    "\n",
    "# experiments dir\n",
    "exp_dir = glob.glob(\"{dir}/experiment*[!.zip]\".format(dir=chosen_dir))[0]\n",
    "\n",
    "# This is a list of all openai dirs, sorted by name (hence, by timestamp)\n",
    "openai_dirs = sorted(glob.glob(\"{dir}/openai*[!.zip]\".format(dir=exp_dir)))\n",
    "\n",
    "# A simple DB of experiments and actions_info.csv should be there\n",
    "dbexp_file = glob.glob(\"{dir}/db_experiments.csv\".format(dir=exp_dir))[0]\n",
    "ainfo_file = glob.glob(\"{dir}/actions_info.csv\".format(dir=exp_dir))[0]\n",
    "config_file = glob.glob(\"{dir}/config*.ini\".format(dir=exp_dir))[0]\n",
    "flog_file = glob.glob(\"{dir}/sl*\".format(dir=chosen_dir))[0]\n",
    "\n",
    "# Make dataframes for the db of experiments and the actions summary\n",
    "dbexp_df = pd.read_csv(dbexp_file)\n",
    "ainfo_df = pd.read_csv(ainfo_file)\n",
    "\n",
    "# Make de target directory\n",
    "import os\n",
    "summaries_dir = \"{exp}/summary\".format(exp=chosen_dir)\n",
    "if not os.path.isdir(summaries_dir):\n",
    "    os.mkdir(summaries_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:50:49.677196Z",
     "start_time": "2019-09-09T11:50:49.511127Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########### BUILD THE RELEVANT DATA FRAMES TO PRINT FOR MAIN SUMMARY ###########\n",
    "################################################################################\n",
    "    \n",
    "# Try to obtain the current times\n",
    "# running_times = search_in_file(flog_file, \".*\\s+(.*)elapsed\")\n",
    "# if len(running_times) == len(openai_dirs):\n",
    "#     f_running_times = []\n",
    "#     for time in running_times:\n",
    "#         time_cleansed = time[0].split(\".\")[0]\n",
    "#         f_running_times.append(time_cleansed)\n",
    "# else:\n",
    "# prev_timestamp = 0\n",
    "# f_running_times = []\n",
    "# for directory in openai_dirs:\n",
    "#     exp_dirname_only = os.path.basename(directory)\n",
    "#     timestamp = os.path.basename(exp_dirname_only.split(\"-\")[1])\n",
    "#     d2 = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "#     if prev_timestamp:  # 2019 05 29 211533\n",
    "#         d1 = datetime.strptime(prev_timestamp, \"%Y%m%d%H%M%S\")\n",
    "#         f_running_times.append(str(d2 - d1))\n",
    "#     prev_timestamp = timestamp\n",
    "# f_running_times.append(\"NA\")\n",
    "\n",
    "openai_dirs_df = pd.DataFrame(zip(openai_dirs, f_running_times), columns=[\"Log directory\", \"Runtime\"])\n",
    "\n",
    "# 4. Search all exceptions\n",
    "exceptions_all = search_in_file(flog_file, \"failed with exception of type.*<(.*)>.*Message.*:\\s*(.*)\")\n",
    "n_exceptions = len(exceptions_all)\n",
    "\n",
    "exceptions_set = set()\n",
    "for error, message in exceptions_all:\n",
    "    exceptions_set.add(error)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "_ = config.read(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "ainfo_df.shape[0]": "8",
     "chosen_dir": "/Users/gomerudo/workspace/thesis_results/6777414",
     "config['DEFAULT']['LogPath']": "/scratch/nodespecific/int1/jgomes/workspaces/workspace_rs_mdn10_omni",
     "config['bash']['Algorithm']": "<p><strong>KeyError</strong>: &#39;Algorithm&#39;</p>\n",
     "config['bash']['Environment']": "<p><strong>KeyError</strong>: &#39;Environment&#39;</p>\n",
     "config['bash']['NSteps']": "<p><strong>KeyError</strong>: &#39;NSteps&#39;</p>\n",
     "config['bash']['Network']": "<p><strong>KeyError</strong>: &#39;Network&#39;</p>\n",
     "config['bash']['NumTimesteps']": "6500",
     "config['metadataset']['DatasetID']": "omniglot",
     "config['metadataset']['TFRecordsRootDir']": "/scratch/nodespecific/int1/jgomes/metadataset_storage/records",
     "config['nasenv.default']['ActionSpaceType']": "chained",
     "config['nasenv.default']['ConfigFile']": "/home/jgomes/workspace/git_storage/nas-dmrl/configs/metadataset_n10/nasenv.yml",
     "config['nasenv.default']['DatasetHandler']": "meta-dataset",
     "config['nasenv.default']['DbFile']": "/scratch/nodespecific/int1/jgomes/workspaces/workspace_rs_mdn10_omni/db_experiments.csv",
     "config['nasenv.default']['MaxSteps']": "100",
     "config['nasenv.default']['TrainerType']": "default",
     "config['trainer.default']['BatchSize']": "128",
     "config['trainer.default']['NEpochs']": "12",
     "config['trainer.tensorflow']['EnableDistributed']": "no",
     "flog_file": "/Users/gomerudo/workspace/thesis_results/6777414/slurm-6777414.out",
     "n_exceptions": "133",
     "openai_dirs_df": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Log directory</th>\n      <th>Runtime</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>",
     "pd.DataFrame(exceptions_set, columns = [\"Error type\"])": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Error type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>class 'ValueError'</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- **Chosen results directory is:** {{chosen_dir}}\n",
    "- **Full log is available at:** {{flog_file}}\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "- **Log Path:** {{config['DEFAULT']['LogPath']}}\n",
    "- **Environment:** {{config['bash']['Environment']}}\n",
    "\n",
    "##### Reinforcement Learning\n",
    "\n",
    "- **Algorithm:** {{config['bash']['Algorithm']}}\n",
    "- **Policy representation:** {{config['bash']['Network']}}\n",
    "- **Number of steps:** {{config['bash']['NSteps']}}\n",
    "- **Total number of timestamps:** {{config['bash']['NumTimesteps']}}\n",
    "- **Number of actions:** {{ainfo_df.shape[0]}}\n",
    "\n",
    "##### NAS details\n",
    "\n",
    "- **Config file:** {{config['nasenv.default']['ConfigFile']}}\n",
    "- **Max Steps:** {{config['nasenv.default']['MaxSteps']}}\n",
    "- **DB of experiments:** {{config['nasenv.default']['DbFile']}}\n",
    "- **Dataset Handler:** {{config['nasenv.default']['DatasetHandler']}}\n",
    "- **Action Space Type:** {{config['nasenv.default']['ActionSpaceType']}}\n",
    "- **Trainer:** {{config['nasenv.default']['TrainerType']}}\n",
    "\n",
    "##### Training details\n",
    "\n",
    "- **Batch size:** {{config['trainer.default']['BatchSize']}}\n",
    "- **Epochs:** {{config['trainer.default']['NEpochs']}}\n",
    "- **Distributed:** {{config['trainer.tensorflow']['EnableDistributed']}}\n",
    "\n",
    "##### Meta-dataset details\n",
    "\n",
    "- **TFRecordsRootDir:** {{config['metadataset']['TFRecordsRootDir']}}\n",
    "- **DatasetID:** {{config['metadataset']['DatasetID']}}\n",
    "\n",
    "#### Individual run directories/time\n",
    "\n",
    "{{openai_dirs_df}}\n",
    "\n",
    "#### Errors found in log while building networks\n",
    "\n",
    "- **Total number of exceptions:** {{n_exceptions}}\n",
    "\n",
    "{{pd.DataFrame(exceptions_set, columns = [\"Error type\"])}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:48:22.703626Z",
     "start_time": "2019-09-09T11:48:22.687193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00\n",
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "def trial_summary(trial_log, include_repeated=True):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    total_runtime = 0 \n",
    "    trial_df = pd.read_csv(trial_log)\n",
    "    trial_df = trial_df[:6000] # Random search only\n",
    "    all_archs = set()\n",
    "    # Iterate the log\n",
    "    for idx, row in trial_df.iterrows():\n",
    "        # Obtain the information information\n",
    "        arch_id = row['composed_id']\n",
    "        running_time = int(row['running_time'])\n",
    "        running_time = 0 if not include_repeated and arch_id in all_archs else running_time\n",
    "        total_runtime += running_time\n",
    "        # add to list at the end\n",
    "        all_archs.add(arch_id)\n",
    "    return total_runtime\n",
    "\n",
    "# Obtain statistics for each trial \n",
    "times = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "        r_time = trial_summary(trial_log, True)\n",
    "        times.append(r_time)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "\n",
    "total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=total_time)))\n",
    "\n",
    "times = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "        r_time = trial_summary(trial_log, False)\n",
    "        times.append(r_time)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "\n",
    "reduced_total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=reduced_total_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T11:50:54.654921Z",
     "start_time": "2019-09-09T11:50:52.901824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 days, 14:34:13\n",
      "4 days, 16:16:44\n"
     ]
    }
   ],
   "source": [
    "def trial_summary(trial_log, include_repeated=True):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    total_runtime = 0 \n",
    "    trial_df = pd.read_csv(trial_log)\n",
    "    trial_df = trial_df[:6000] # Random search only\n",
    "    all_archs = set()\n",
    "    # Iterate the log\n",
    "    for idx, row in trial_df.iterrows():\n",
    "        # Obtain the information information\n",
    "        arch_id = row['composed_id']\n",
    "        running_time = int(row['running_time'])\n",
    "        running_time = 0 if not include_repeated and arch_id in all_archs else running_time\n",
    "        total_runtime += running_time\n",
    "        # add to list at the end\n",
    "        all_archs.add(arch_id)\n",
    "    return total_runtime\n",
    "\n",
    "# Obtain statistics for each trial \n",
    "times = []\n",
    "# for i, openai_dir in enumerate(openai_dirs):\n",
    "try:\n",
    "    trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=exp_dir)))[0]\n",
    "    r_time = trial_summary(trial_log, True)\n",
    "    times.append(r_time)\n",
    "except IndexError:\n",
    "    print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "#         pass\n",
    "\n",
    "total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=total_time)))\n",
    "\n",
    "times = []\n",
    "# for i, openai_dir in enumerate(openai_dirs):\n",
    "try:\n",
    "#         trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "    trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=exp_dir)))[0]\n",
    "    r_time = trial_summary(trial_log, False)\n",
    "    times.append(r_time)\n",
    "except IndexError:\n",
    "    print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "    pass\n",
    "\n",
    "reduced_total_time = sum(times)\n",
    "print(str(datetime.timedelta(seconds=reduced_total_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
