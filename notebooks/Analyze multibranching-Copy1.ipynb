{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze multi-branching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T12:53:38.261995Z",
     "start_time": "2019-09-16T12:53:37.604945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import for interactive notebook (see:\n",
    "# https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html)\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "\n",
    "\n",
    "# Import to list files in directories\n",
    "import glob\n",
    "\n",
    "# Import for regular expressions\n",
    "import re\n",
    "\n",
    "# Imports for path operations\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# For date operations\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', -1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import configparser\n",
    "\n",
    "# import jtplot module in notebook\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "jtplot.style(theme='onedork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:04:10.741672Z",
     "start_time": "2019-09-16T13:04:10.684592Z"
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"/Users/gomerudo/workspace/thesis_results\"\n",
    "\n",
    "def rettext(text):\n",
    "    return text\n",
    "\n",
    "def search_in_file(file, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    results = []\n",
    "    for i, line in enumerate(open(file)):\n",
    "        for match in re.finditer(pattern, line):\n",
    "            results.append(match.groups())\n",
    "    return results\n",
    "\n",
    "form_item_layout = Layout(\n",
    "    width=\"50%\"\n",
    ")\n",
    "\n",
    "w_resdirs = interactive(\n",
    "    rettext,\n",
    "#     text=sorted(glob.glob(\"{dir}/[mix-]?[0-9]*\".format(dir=RESULTS_DIR))),\n",
    "    text=sorted(glob.glob(\"{dir}/*\".format(dir=RESULTS_DIR))),\n",
    "    layout=form_item_layout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:04:11.876512Z",
     "start_time": "2019-09-16T13:04:11.850217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874cdc5e62ec44ecb4fd6a5c92e20059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text', options=('/Users/gomerudo/workspace/thesis_results/27969', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w_resdirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:04:21.419550Z",
     "start_time": "2019-09-16T13:04:21.296557Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############ OBTAIN THE FILES AND DIRECTORIES TO QUERY FOR ANALYSIS ############\n",
    "################################################################################\n",
    "\n",
    "# Obtain the chosen directory\n",
    "chosen_dir = w_resdirs.result\n",
    "\n",
    "# experiments dir\n",
    "exp_dir = glob.glob(\"{dir}/experiment*[!.zip]\".format(dir=chosen_dir))[0]\n",
    "\n",
    "# This is a list of all openai dirs, sorted by name (hence, by timestamp)\n",
    "openai_dirs = sorted(glob.glob(\"{dir}/openai*[!.zip]\".format(dir=exp_dir)))\n",
    "\n",
    "# A simple DB of experiments and actions_info.csv should be there\n",
    "dbexp_file = glob.glob(\"{dir}/db_experiments.csv\".format(dir=exp_dir))[0]\n",
    "ainfo_file = glob.glob(\"{dir}/actions_info.csv\".format(dir=exp_dir))[0]\n",
    "config_file = glob.glob(\"{dir}/config*.ini\".format(dir=exp_dir))[0]\n",
    "flog_file = glob.glob(\"{dir}/sl*\".format(dir=chosen_dir))[0]\n",
    "\n",
    "# Make dataframes for the db of experiments and the actions summary\n",
    "dbexp_df = pd.read_csv(dbexp_file)\n",
    "ainfo_df = pd.read_csv(ainfo_file)\n",
    "\n",
    "# Make de target directory\n",
    "import os\n",
    "summaries_dir = \"{exp}/summary\".format(exp=chosen_dir)\n",
    "if not os.path.isdir(summaries_dir):\n",
    "    os.mkdir(summaries_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:04:31.269613Z",
     "start_time": "2019-09-16T13:04:31.058783Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########### BUILD THE RELEVANT DATA FRAMES TO PRINT FOR MAIN SUMMARY ###########\n",
    "################################################################################\n",
    "    \n",
    "# Try to obtain the current times\n",
    "# running_times = search_in_file(flog_file, \".*\\s+(.*)elapsed\")\n",
    "# if len(running_times) == len(openai_dirs):\n",
    "#     f_running_times = []\n",
    "#     for time in running_times:\n",
    "#         time_cleansed = time[0].split(\".\")[0]\n",
    "#         f_running_times.append(time_cleansed)\n",
    "# else:\n",
    "prev_timestamp = 0\n",
    "f_running_times = []\n",
    "for directory in openai_dirs:\n",
    "    exp_dirname_only = os.path.basename(directory)\n",
    "    timestamp = os.path.basename(exp_dirname_only.split(\"-\")[1])\n",
    "    d2 = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "    if prev_timestamp:  # 2019 05 29 211533\n",
    "        d1 = datetime.strptime(prev_timestamp, \"%Y%m%d%H%M%S\")\n",
    "        f_running_times.append(str(d2 - d1))\n",
    "    prev_timestamp = timestamp\n",
    "f_running_times.append(\"NA\")\n",
    "\n",
    "openai_dirs_df = pd.DataFrame(zip(openai_dirs, f_running_times), columns=[\"Log directory\", \"Runtime\"])\n",
    "\n",
    "# 4. Search all exceptions\n",
    "exceptions_all = search_in_file(flog_file, \"failed with exception of type.*<(.*)>.*Message.*:\\s*(.*)\")\n",
    "n_exceptions = len(exceptions_all)\n",
    "\n",
    "exceptions_set = set()\n",
    "for error, message in exceptions_all:\n",
    "    exceptions_set.add(error)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "_ = config.read(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "ainfo_df.shape[0]": "14",
     "chosen_dir": "/Users/gomerudo/workspace/thesis_results/32625",
     "config['DEFAULT']['LogPath']": "/home/TUE/20175601/workspace/git_storage/openai-baselines/workspace_mdn10_mb",
     "config['bash']['Algorithm']": "meta_a2c",
     "config['bash']['Environment']": "NAS_cifar10-v1",
     "config['bash']['NSteps']": "10",
     "config['bash']['Network']": "meta_lstm",
     "config['bash']['NumTimesteps']": "20000",
     "config['metadataset']['DatasetID']": "omniglot",
     "config['metadataset']['TFRecordsRootDir']": "/home/TUE/20175601/workspace/metadataset_storage/records",
     "config['nasenv.default']['ActionSpaceType']": "pred-free",
     "config['nasenv.default']['ConfigFile']": "/home/TUE/20175601/workspace/git_storage/nas-dmrl/configs/metadataset_n10/nasenv-mb.yml",
     "config['nasenv.default']['DatasetHandler']": "meta-dataset",
     "config['nasenv.default']['DbFile']": "/home/TUE/20175601/workspace/git_storage/openai-baselines/workspace_mdn10_mb/db_experiments.csv",
     "config['nasenv.default']['MaxSteps']": "20",
     "config['nasenv.default']['TrainerType']": "default",
     "config['trainer.default']['BatchSize']": "64",
     "config['trainer.default']['NEpochs']": "12",
     "config['trainer.tensorflow']['EnableDistributed']": "no",
     "flog_file": "/Users/gomerudo/workspace/thesis_results/32625/slurm-32625.out",
     "n_exceptions": "18",
     "openai_dirs_df": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Log directory</th>\n      <th>Runtime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/Users/gomerudo/workspace/thesis_results/32625/experiment-20190914190430/openai-20190914190432</td>\n      <td>NA</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
     "pd.DataFrame(exceptions_set, columns = [\"Error type\"])": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Error type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>class 'ValueError'</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- **Chosen results directory is:** {{chosen_dir}}\n",
    "- **Full log is available at:** {{flog_file}}\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "- **Log Path:** {{config['DEFAULT']['LogPath']}}\n",
    "- **Environment:** {{config['bash']['Environment']}}\n",
    "\n",
    "##### Reinforcement Learning\n",
    "\n",
    "- **Algorithm:** {{config['bash']['Algorithm']}}\n",
    "- **Policy representation:** {{config['bash']['Network']}}\n",
    "- **Number of steps:** {{config['bash']['NSteps']}}\n",
    "- **Total number of timestamps:** {{config['bash']['NumTimesteps']}}\n",
    "- **Number of actions:** {{ainfo_df.shape[0]}}\n",
    "\n",
    "##### NAS details\n",
    "\n",
    "- **Config file:** {{config['nasenv.default']['ConfigFile']}}\n",
    "- **Max Steps:** {{config['nasenv.default']['MaxSteps']}}\n",
    "- **DB of experiments:** {{config['nasenv.default']['DbFile']}}\n",
    "- **Dataset Handler:** {{config['nasenv.default']['DatasetHandler']}}\n",
    "- **Action Space Type:** {{config['nasenv.default']['ActionSpaceType']}}\n",
    "- **Trainer:** {{config['nasenv.default']['TrainerType']}}\n",
    "\n",
    "##### Training details\n",
    "\n",
    "- **Batch size:** {{config['trainer.default']['BatchSize']}}\n",
    "- **Epochs:** {{config['trainer.default']['NEpochs']}}\n",
    "- **Distributed:** {{config['trainer.tensorflow']['EnableDistributed']}}\n",
    "\n",
    "##### Meta-dataset details\n",
    "\n",
    "- **TFRecordsRootDir:** {{config['metadataset']['TFRecordsRootDir']}}\n",
    "- **DatasetID:** {{config['metadataset']['DatasetID']}}\n",
    "\n",
    "#### Individual run directories/time\n",
    "\n",
    "{{openai_dirs_df}}\n",
    "\n",
    "#### Errors found in log while building networks\n",
    "\n",
    "- **Total number of exceptions:** {{n_exceptions}}\n",
    "\n",
    "{{pd.DataFrame(exceptions_set, columns = [\"Error type\"])}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T12:54:41.645585Z",
     "start_time": "2019-09-16T12:54:41.190854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arch is omniglot-None\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8131484d8f68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mbest_architecture_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{d}-{h}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DatasetID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_global_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best arch is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_architecture_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mbest_architecture_dbexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbexp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdbexp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset-nethash'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbest_architecture_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/pyvenv/nas-rl2/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pyvenv/nas-rl2/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pyvenv/nas-rl2/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2137\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "def trial_summary(trial_log):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    try:\n",
    "        # Read the log file\n",
    "        trial_df = pd.read_csv(trial_log)\n",
    "        actions_distribution = [0]*ainfo_df.shape[0]\n",
    "        \n",
    "        # CONTROL VARIABLES\n",
    "        # Info for the best architecture\n",
    "        best_architecture = None\n",
    "        best_reward = -1\n",
    "\n",
    "        # Aux for episode control\n",
    "        max_step_ep = 0\n",
    "        best_reward_ep = 0\n",
    "\n",
    "        # History lists\n",
    "        max_step_count_history = []\n",
    "        best_reward_history = []\n",
    "        all_rewards_history = []\n",
    "\n",
    "        # Accumulated rewards per trial\n",
    "        acc_rewards_history = []\n",
    "        acc_reward = 0\n",
    "        # Iterate the log\n",
    "        for idx, row in trial_df.iterrows():\n",
    "            \n",
    "            # Obtain the information information\n",
    "            action_id = int(row['action_id'])\n",
    "            step = int(row['step_count'])\n",
    "            is_valid = bool(row['valid'])\n",
    "            arch_hash = row['end_state_hashed']\n",
    "            reward = float(row['reward'])\n",
    "            \n",
    "            # THIS SECTION IS FOR THE \"OVERALL\" STATISTICS IN TRIAL\n",
    "            # a) Add information to the distribution of actions\n",
    "            actions_distribution[action_id] += 1\n",
    "    \n",
    "            # b) Get the best reward by comparing one by one\n",
    "            if reward > best_reward:\n",
    "                best_reward = reward\n",
    "                best_architecture = arch_hash\n",
    "\n",
    "            # c) History of all rewards in trial\n",
    "            all_rewards_history.append(reward)\n",
    "            \n",
    "            # d) Unique architectures\n",
    "            unique_architectures.add(arch_hash)\n",
    "\n",
    "            # THIS SECTION IS FOR THE EPISODE STATISTICS\n",
    "            if step > max_step_ep:\n",
    "                max_step_ep = step\n",
    "                best_reward_ep = reward if reward > best_reward_ep else best_reward_ep\n",
    "                acc_reward += reward\n",
    "            # Otherwise, append the best information we read\n",
    "            else:\n",
    "                max_step_count_history.append(max_step_ep)\n",
    "                best_reward_history.append(best_reward_ep)\n",
    "                acc_rewards_history.append(acc_reward)\n",
    "                max_step_ep = step\n",
    "                best_reward_ep = reward\n",
    "                acc_reward = reward\n",
    "    except Exception:\n",
    "        pass\n",
    "    finally:\n",
    "        return {\n",
    "            'actions_distribution': actions_distribution,\n",
    "            'max_step_history': max_step_count_history,\n",
    "            'best_reward_history': best_reward_history,\n",
    "            'all_rewards_history': all_rewards_history,\n",
    "            'best_architecture': best_architecture,\n",
    "            'best_reward': best_reward,\n",
    "            'n_episodes': len(best_reward_history),\n",
    "            'unique_architectures': set(trial_df['end_state_hashed'].unique()),\n",
    "            'acc_rewards_history': acc_rewards_history,\n",
    "        }\n",
    "\n",
    "# Obtain statistics for each trial \n",
    "stats = []\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "        info_trial = trial_summary(trial_log)\n",
    "        stats.append(info_trial)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "    \n",
    "# Build global statistics for the whole experiment\n",
    "n_episodes_history = []\n",
    "unique_architectures = set()\n",
    "last_length_set_archs = len(unique_architectures)\n",
    "best_global_architecture = None\n",
    "best_global_reward = 0\n",
    "\n",
    "global_best_reward_history = []\n",
    "global_all_rewards_history = []\n",
    "global_max_step_history = []\n",
    "new_archs_history = []\n",
    "\n",
    "for trial_stats in stats:\n",
    "    # Miscellaneous\n",
    "    n_episodes_history.append(len(trial_stats['best_reward_history']))\n",
    "    unique_architectures.update(trial_stats['unique_architectures'])\n",
    "    new_sampled_architectures = len(unique_architectures) - last_length_set_archs\n",
    "    last_length_set_archs = len(unique_architectures)\n",
    "    new_archs_history.append(new_sampled_architectures)\n",
    "    \n",
    "    # Best values\n",
    "    if trial_stats['best_reward'] > best_global_reward:\n",
    "        best_global_reward = trial_stats['best_reward']\n",
    "        best_global_architecture = trial_stats['best_architecture']\n",
    "        \n",
    "    # Global histories\n",
    "    global_best_reward_history += trial_stats['best_reward_history']\n",
    "    global_max_step_history += trial_stats['max_step_history']\n",
    "    \n",
    "    \n",
    "    # The distribution of actions\n",
    "total_n_episodes = sum(n_episodes_history)\n",
    "\n",
    "# Search for the best architecture\n",
    "best_architecture_id = \"{d}-{h}\".format(d=config['metadataset']['DatasetID'], h=best_global_architecture)\n",
    "print(\"Best arch is\", best_architecture_id)\n",
    "best_architecture_dbexp = dbexp_df.loc[dbexp_df['dataset-nethash'] == best_architecture_id].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:04:41.671499Z",
     "start_time": "2019-09-16T13:04:38.911548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'narchs': 9772, 'steps': 20000, 'count-mb': 1106}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_multibranch(arch):\n",
    "    last_seen = 0\n",
    "    for row in arch:\n",
    "        pred = row[3]\n",
    "        if pred != 0 and pred != last_seen + 1:\n",
    "            return True\n",
    "        last_seen = pred\n",
    "    return False\n",
    "\n",
    "def trial_summary(trial_log):\n",
    "    # Read in try catch because the file can be corrupted or might not exist\n",
    "    info = None\n",
    "    trial_df = pd.read_csv(trial_log)\n",
    "    visited = set()\n",
    "    mb_set = set()\n",
    "    count_mb = 0\n",
    "    for idx, row in trial_df.iterrows():\n",
    "        # Obtain the information\n",
    "        arch_id = row['composed_id']\n",
    "        if arch_id in visited:\n",
    "            pass\n",
    "        else:\n",
    "            visited.add(arch_id)\n",
    "            res_arch = row['end_state']\n",
    "            arch_formatted = res_arch.replace(\"[\", \"\").replace(\"]\", \" \").replace(\"\\n\", \"\")\n",
    "            narray = np.fromstring(arch_formatted, dtype=int, sep=' ')\n",
    "            narray = narray.reshape((10, 5))\n",
    "            if is_multibranch(narray):\n",
    "                mb_set.add(arch_id)\n",
    "                count_mb += 1\n",
    "#             break\n",
    "    info = {\n",
    "        'narchs': len(visited),\n",
    "        'steps': trial_df.shape[0],\n",
    "        'count-mb': count_mb\n",
    "    }\n",
    "    return info, mb_set\n",
    "\n",
    "for i, openai_dir in enumerate(openai_dirs):\n",
    "    try:\n",
    "        trial_log = sorted(glob.glob(\"{dir}/episode_logs/*\".format(dir=openai_dir)))[0]\n",
    "        summary, mb_archs = trial_summary(trial_log)\n",
    "    except IndexError:\n",
    "        print(\"Could not read the episode_logs in {}\".format(openai_dir))\n",
    "        pass\n",
    "    \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
